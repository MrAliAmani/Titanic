# -*- coding: utf-8 -*-
"""titanic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m5mFgCLMqlG0jhOqpaSkfSA8hr13ntuO
"""

import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Load the dataset
train_data = pd.read_csv("https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv")

# Check for missing values
print(train_data.isnull().sum())

# Replace missing values with the median
train_data.fillna(train_data.median(), inplace=True)

# Check for outliers in the 'Age' column
q1 = train_data['Age'].quantile(0.25)
q3 = train_data['Age'].quantile(0.75)
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr
outliers = train_data[(train_data['Age'] < lower_bound) | (train_data['Age'] > upper_bound)]
print(outliers)

# Remove the outliers
train_data = train_data[(train_data['Age'] >= lower_bound) & (train_data['Age'] <= upper_bound)]

# Convert non-numeric features to numeric representations using label encoding
le = LabelEncoder()
train_data['Sex'] = le.fit_transform(train_data['Sex'])
train_data['Embarked'] = le.fit_transform(train_data['Embarked'])

# Perform one-hot encoding on the 'Pclass' column
train_data = pd.get_dummies(train_data, columns=['Pclass'])

# Remove the string columns from the data
train_data = train_data.drop(['Name', 'Ticket', 'Cabin'], axis=1)

# Split the data into training and validation sets
train_dataset, val_dataset = train_test_split(train_data, test_size=0.3)

# Split the data into input and target
train_target = train_dataset.pop("Survived").replace({'0': 0, '1': 1}).astype('float32')
val_target = val_dataset.pop("Survived").replace({'0': 0, '1': 1}).astype('float32')

# Prepare the data for training
train_dataset = tf.data.Dataset.from_tensor_slices((train_dataset.values, train_target.values))
train_dataset = train_dataset.shuffle(len(train_dataset)).batch(32)
val_dataset = tf.data.Dataset.from_tensor_slices((val_dataset.values, val_target.values))
val_dataset = val_dataset.batch(32)

# Define the model architecture
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation="relu", input_shape=(10,)),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(128, activation="relu"),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(64, activation="relu"),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(64, activation="relu"),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(32, activation="relu"),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1, activation='sigmoid')
])


# Compile the model
model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer="adam", metrics=["accuracy"])

# Define early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)

# Train the model
history = model.fit(train_dataset, epochs=600, validation_data=val_dataset, callbacks=[early_stopping])

# Plot the training history
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.callbacks import EarlyStopping, TensorBoard

# Load the dataset
train_data = pd.read_csv("https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv")

# Check for missing values
print(train_data.isnull().sum())

# Replace missing values with the median
train_data.fillna(train_data.median(), inplace=True)

# Check for outliers in the 'Age' column
q1 = train_data['Age'].quantile(0.25)
q3 = train_data['Age'].quantile(0.75)
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr
outliers = train_data[(train_data['Age'] < lower_bound) | (train_data['Age'] > upper_bound)]
print(outliers)

# Remove the outliers
train_data = train_data[(train_data['Age'] >= lower_bound) & (train_data['Age'] <= upper_bound)]

# Drop non-numeric columns
train_data = train_data.drop(columns=['Name', 'Ticket', 'Cabin'])

# Convert non-numeric features to numeric representations using label encoding
le = LabelEncoder()
train_data['Sex'] = le.fit_transform(train_data['Sex'])
train_data['Embarked'] = le.fit_transform(train_data['Embarked'])

# Perform one-hot encoding on the 'Pclass' column
train_data = pd.get_dummies(train_data, columns=['Pclass'])

# Split the data into training and validation sets
train_dataset, val_dataset = train_test_split(train_data, test_size=0.2)

# Split the data into input and target
train_target = train_dataset.pop("Survived").replace({'0': 0, '1': 1}).astype('float32')
val_target = val_dataset.pop("Survived").replace({'0': 0, '1': 1}).astype('float32')

# Prepare the data for training
train_dataset = tf.data.Dataset.from_tensor_slices((train_dataset.values, train_target.values))
train_dataset = train_dataset.shuffle(len(train_dataset)).batch(32)
val_dataset = tf.data.Dataset.from_tensor_slices((val_dataset.values, val_target.values))
val_dataset = val_dataset.batch(32)

# Define the model architecture
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation="relu", input_shape=(train_data.shape[1],)),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(128, activation="relu"),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(64, activation="relu"),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(64, activation="relu"),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(32, activation="relu"),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1, activation=None)
])

# Compile the model
model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer="adam", metrics=["accuracy"])

# Set up callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=10)
tensorboard = TensorBoard(log_dir='./logs', histogram_freq=1)

# Train the model
history = model.fit(train_dataset, epochs=100, validation_data=val_dataset, callbacks=[early_stopping, tensorboard])

# Plot
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

